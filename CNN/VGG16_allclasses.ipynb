{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPkspt5pQHnn",
        "outputId": "7d7b03e4-687f-4e3e-a90b-1cbf9aaef237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up constants\n",
        "IMG_SIZE = (224, 224)  # VGG16 input size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100  # Increased max epochs, early stopping will prevent unnecessary training\n",
        "\n",
        "# Define the path to the images folder\n",
        "data_dir = '/content/drive/My Drive/images'\n",
        "print(\"Contents of data_dir:\")\n",
        "print(os.listdir(data_dir))\n",
        "\n",
        "# Update class names to include SCC\n",
        "class_names = ['MEL', 'NV', 'BCC', 'SCC']\n",
        "for class_name in class_names:\n",
        "    if not os.path.isdir(os.path.join(data_dir, class_name)):\n",
        "        raise ValueError(f\"Folder {class_name} not found in {data_dir}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gE7Uu9OQPl9",
        "outputId": "d81b2e47-67d9-434e-c3f6-f76cef573d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of data_dir:\n",
            "['NV', 'MEL', 'BCC', 'SCC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up data generators with increased augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    classes=class_names,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    classes=class_names,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z09riiWZQSvt",
        "outputId": "d46e423f-6cb9-4ade-dbd1-2611abc86ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7190 images belonging to 4 classes.\n",
            "Found 1795 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(4, activation='softmax')(x)  # 4 classes now\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n"
      ],
      "metadata": {
        "id": "SM5Lm5eoQXQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define F1 Score metric as a class\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n"
      ],
      "metadata": {
        "id": "n_o2LDvAQavi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with additional metrics\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Define callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with class weights and callbacks\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/drive/My Drive/models/skin_lesion_classifier_vgg16_all_classes.h5')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3RnygZsQd4g",
        "outputId": "1c32bbc6-7b17-4885-c21a-4dbdcdaf682b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2261s\u001b[0m 10s/step - accuracy: 0.2788 - f1_score: 0.0047 - loss: 1.3824 - precision: 0.1822 - recall: 0.0024 - val_accuracy: 0.0703 - val_f1_score: 0.0000e+00 - val_loss: 1.4114 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m  1/224\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 466ms/step - accuracy: 0.2500 - f1_score: 0.0000e+00 - loss: 1.8435 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.2500 - f1_score: 0.0000e+00 - loss: 1.8435 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_f1_score: 0.0000e+00 - val_loss: 1.4354 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 1s/step - accuracy: 0.3725 - f1_score: 0.0388 - loss: 1.2885 - precision: 0.7009 - recall: 0.0200 - val_accuracy: 0.0826 - val_f1_score: 0.0000e+00 - val_loss: 1.4094 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 156ms/step - accuracy: 0.0938 - f1_score: 0.0588 - loss: 1.5047 - precision: 0.5000 - recall: 0.0312 - val_accuracy: 0.0000e+00 - val_f1_score: 0.0000e+00 - val_loss: 1.3819 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 1s/step - accuracy: 0.3348 - f1_score: 0.0427 - loss: 1.2737 - precision: 0.7409 - recall: 0.0220 - val_accuracy: 0.6975 - val_f1_score: 0.0000e+00 - val_loss: 1.2190 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245us/step - accuracy: 0.5625 - f1_score: 0.0606 - loss: 1.4617 - precision: 1.0000 - recall: 0.0312 - val_accuracy: 0.6667 - val_f1_score: 0.0000e+00 - val_loss: 1.2581 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 1s/step - accuracy: 0.6000 - f1_score: 0.1348 - loss: 1.2181 - precision: 0.7428 - recall: 0.0747 - val_accuracy: 0.3527 - val_f1_score: 0.0011 - val_loss: 1.3284 - val_precision: 0.5000 - val_recall: 5.5804e-04 - learning_rate: 1.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253us/step - accuracy: 0.3750 - f1_score: 0.0000e+00 - loss: 1.8872 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.6667 - val_f1_score: 0.0000e+00 - val_loss: 1.3022 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 1s/step - accuracy: 0.4628 - f1_score: 0.0867 - loss: 1.2136 - precision: 0.7364 - recall: 0.0466 - val_accuracy: 0.7104 - val_f1_score: 0.4818 - val_loss: 1.0128 - val_precision: 0.9930 - val_recall: 0.3181 - learning_rate: 1.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 435us/step - accuracy: 0.6562 - f1_score: 0.2222 - loss: 0.9655 - precision: 1.0000 - recall: 0.1250 - val_accuracy: 0.6667 - val_f1_score: 0.0000e+00 - val_loss: 0.9635 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 1s/step - accuracy: 0.5790 - f1_score: 0.3891 - loss: 1.1025 - precision: 0.8923 - recall: 0.2556 - val_accuracy: 0.6094 - val_f1_score: 0.6295 - val_loss: 0.9982 - val_precision: 0.9311 - val_recall: 0.4754 - learning_rate: 1.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 172ms/step - accuracy: 0.4688 - f1_score: 0.4651 - loss: 1.5791 - precision: 0.9091 - recall: 0.3125 - val_accuracy: 0.3333 - val_f1_score: 0.4000 - val_loss: 1.0954 - val_precision: 0.5000 - val_recall: 0.3333 - learning_rate: 1.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 1s/step - accuracy: 0.5497 - f1_score: 0.5833 - loss: 1.0051 - precision: 0.9097 - recall: 0.4293 - val_accuracy: 0.5748 - val_f1_score: 0.5792 - val_loss: 1.0775 - val_precision: 0.9788 - val_recall: 0.4113 - learning_rate: 1.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394us/step - accuracy: 0.5000 - f1_score: 0.5778 - loss: 1.0272 - precision: 1.0000 - recall: 0.4062 - val_accuracy: 0.6667 - val_f1_score: 0.5000 - val_loss: 0.7715 - val_precision: 1.0000 - val_recall: 0.3333 - learning_rate: 1.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 1s/step - accuracy: 0.5352 - f1_score: 0.5670 - loss: 1.0333 - precision: 0.8689 - recall: 0.4210 - val_accuracy: 0.5921 - val_f1_score: 0.6373 - val_loss: 0.8803 - val_precision: 0.9596 - val_recall: 0.4771 - learning_rate: 1.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 182ms/step - accuracy: 0.4062 - f1_score: 0.4783 - loss: 0.7790 - precision: 0.7857 - recall: 0.3438 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.1492 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 998ms/step - accuracy: 0.5409 - f1_score: 0.5802 - loss: 0.9891 - precision: 0.8840 - recall: 0.4321 - val_accuracy: 0.5792 - val_f1_score: 0.6563 - val_loss: 0.8309 - val_precision: 0.9695 - val_recall: 0.4961 - learning_rate: 1.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step - accuracy: 0.5312 - f1_score: 0.5455 - loss: 1.0902 - precision: 1.0000 - recall: 0.3750 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0061 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 1s/step - accuracy: 0.5387 - f1_score: 0.5800 - loss: 0.9530 - precision: 0.8887 - recall: 0.4306 - val_accuracy: 0.5977 - val_f1_score: 0.6570 - val_loss: 0.8223 - val_precision: 0.9417 - val_recall: 0.5045 - learning_rate: 1.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7188 - f1_score: 0.6939 - loss: 0.7048 - precision: 1.0000 - recall: 0.5312 - val_accuracy: 0.3333 - val_f1_score: 0.0000e+00 - val_loss: 1.5434 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 1.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 1s/step - accuracy: 0.5718 - f1_score: 0.5989 - loss: 0.9059 - precision: 0.8106 - recall: 0.4753 - val_accuracy: 0.6328 - val_f1_score: 0.6507 - val_loss: 0.8131 - val_precision: 0.7359 - val_recall: 0.5831 - learning_rate: 1.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247us/step - accuracy: 0.5938 - f1_score: 0.6316 - loss: 0.6846 - precision: 0.7200 - recall: 0.5625 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0186 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 1.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 1s/step - accuracy: 0.5914 - f1_score: 0.6074 - loss: 0.7531 - precision: 0.6934 - recall: 0.5405 - val_accuracy: 0.5240 - val_f1_score: 0.5253 - val_loss: 0.9638 - val_precision: 0.5817 - val_recall: 0.4788 - learning_rate: 1.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 179ms/step - accuracy: 0.5000 - f1_score: 0.5333 - loss: 0.7893 - precision: 0.5714 - recall: 0.5000 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.4448 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 1s/step - accuracy: 0.6140 - f1_score: 0.6168 - loss: 0.6764 - precision: 0.7037 - recall: 0.5491 - val_accuracy: 0.6691 - val_f1_score: 0.6793 - val_loss: 0.6853 - val_precision: 0.7393 - val_recall: 0.6283 - learning_rate: 2.0000e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 180ms/step - accuracy: 0.7188 - f1_score: 0.7797 - loss: 0.4427 - precision: 0.8519 - recall: 0.7188 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 0.7868 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 2.0000e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 1s/step - accuracy: 0.6310 - f1_score: 0.6413 - loss: 0.6459 - precision: 0.7002 - recall: 0.5916 - val_accuracy: 0.6629 - val_f1_score: 0.6720 - val_loss: 0.6960 - val_precision: 0.7223 - val_recall: 0.6283 - learning_rate: 2.0000e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417us/step - accuracy: 0.6875 - f1_score: 0.7018 - loss: 0.3873 - precision: 0.8000 - recall: 0.6250 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0034 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 1s/step - accuracy: 0.6429 - f1_score: 0.6484 - loss: 0.6187 - precision: 0.7060 - recall: 0.5996 - val_accuracy: 0.6635 - val_f1_score: 0.6659 - val_loss: 0.7047 - val_precision: 0.7068 - val_recall: 0.6295 - learning_rate: 2.0000e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 229us/step - accuracy: 0.6562 - f1_score: 0.7119 - loss: 0.6603 - precision: 0.7778 - recall: 0.6562 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.2003 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 31/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 1s/step - accuracy: 0.6449 - f1_score: 0.6464 - loss: 0.5952 - precision: 0.6990 - recall: 0.6013 - val_accuracy: 0.6574 - val_f1_score: 0.6643 - val_loss: 0.7198 - val_precision: 0.7060 - val_recall: 0.6272 - learning_rate: 2.0000e-05\n",
            "Epoch 32/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231us/step - accuracy: 0.7500 - f1_score: 0.7000 - loss: 0.4870 - precision: 0.7500 - recall: 0.6562 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0873 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 2.0000e-05\n",
            "Epoch 33/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 1s/step - accuracy: 0.6775 - f1_score: 0.6763 - loss: 0.5760 - precision: 0.7260 - recall: 0.6330 - val_accuracy: 0.6881 - val_f1_score: 0.6962 - val_loss: 0.6511 - val_precision: 0.7351 - val_recall: 0.6613 - learning_rate: 2.0000e-05\n",
            "Epoch 34/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 310us/step - accuracy: 0.5938 - f1_score: 0.6102 - loss: 0.7579 - precision: 0.6667 - recall: 0.5625 - val_accuracy: 1.0000 - val_f1_score: 0.8000 - val_loss: 0.4119 - val_precision: 1.0000 - val_recall: 0.6667 - learning_rate: 4.0000e-06\n",
            "Epoch 35/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 1s/step - accuracy: 0.6782 - f1_score: 0.6775 - loss: 0.5648 - precision: 0.7231 - recall: 0.6374 - val_accuracy: 0.6936 - val_f1_score: 0.7017 - val_loss: 0.6560 - val_precision: 0.7364 - val_recall: 0.6702 - learning_rate: 4.0000e-06\n",
            "Epoch 36/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 178ms/step - accuracy: 0.7188 - f1_score: 0.7097 - loss: 1.0099 - precision: 0.7333 - recall: 0.6875 - val_accuracy: 0.3333 - val_f1_score: 0.4000 - val_loss: 1.1917 - val_precision: 0.5000 - val_recall: 0.3333 - learning_rate: 4.0000e-06\n",
            "Epoch 37/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 1s/step - accuracy: 0.6876 - f1_score: 0.6857 - loss: 0.5825 - precision: 0.7315 - recall: 0.6454 - val_accuracy: 0.6998 - val_f1_score: 0.7050 - val_loss: 0.6550 - val_precision: 0.7356 - val_recall: 0.6769 - learning_rate: 4.0000e-06\n",
            "Epoch 38/100\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 181ms/step - accuracy: 0.8438 - f1_score: 0.8065 - loss: 0.3857 - precision: 0.8333 - recall: 0.7812 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 1.0105 - val_precision: 0.6667 - val_recall: 0.6667 - learning_rate: 4.0000e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print class indices\n",
        "print(\"Class indices:\", train_generator.class_indices)\n",
        "\n",
        "# Function to predict image\n",
        "def predict_image(img_path, nv_threshold=0.7):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "\n",
        "    if np.argmax(prediction) == class_names.index('NV') and prediction[0][class_names.index('NV')] < nv_threshold:\n",
        "        predicted_class = class_names[np.argsort(prediction[0])[-2]]\n",
        "    else:\n",
        "        predicted_class = class_names[np.argmax(prediction)]\n",
        "\n",
        "    confidence = np.max(prediction)\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Directory containing test images\n",
        "finish_dir = '/content/drive/My Drive/finish'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7RKCr2SQheG",
        "outputId": "b467d62c-d498-4e7e-e2ce-afa15288a384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class indices: {'MEL': 0, 'NV': 1, 'BCC': 2, 'SCC': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive prediction loop\n",
        "while True:\n",
        "    user_input = input(\"Enter an image number (1-1000) or 'q' to quit: \")\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        image_number = int(user_input)\n",
        "\n",
        "        for filename in os.listdir(finish_dir):\n",
        "            if filename.startswith(f\"{image_number}.\") and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(finish_dir, filename)\n",
        "\n",
        "                predicted_class, confidence = predict_image(img_path)\n",
        "\n",
        "                print(f\"Image: {filename}\")\n",
        "                print(f\"Predicted class: {predicted_class}\")\n",
        "                print(f\"Confidence: {confidence:.2f}\")\n",
        "                print()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found with number {image_number}\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number or 'q' to quit.\")\n",
        "\n",
        "print(\"Thank you for using the classifier!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q99oc0xIQmQm",
        "outputId": "b22ff6f6-2395-4531-dd07-184aaaf0c217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an image number (1-1000) or 'q' to quit: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Image: 1.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.69\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 2.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.67\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Image: 3.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.55\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 4.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.71\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Image: 5.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.66\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 6\n",
            "No image found with number 6\n",
            "Enter an image number (1-1000) or 'q' to quit: 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 6.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.66\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 7.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.79\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 8.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.43\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 9\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Image: 9.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.68\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 10.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.67\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 11.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.55\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 12\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Image: 12.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.69\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 13\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Image: 13.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.76\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 1.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.69\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 2.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.78\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 3.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.57\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Image: 4.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.63\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 5.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.80\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 6.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.86\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 7\n",
            "No image found with number 7\n",
            "Enter an image number (1-1000) or 'q' to quit: 8\n",
            "No image found with number 8\n",
            "Enter an image number (1-1000) or 'q' to quit: 9\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Image: 9.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.38\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 8.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.73\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 7.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.51\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 10.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.42\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 11.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.79\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 12\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 12.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.73\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 13\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Image: 13.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.80\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 14\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 14.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.76\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 1.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.45\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 2.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.60\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Image: 3.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.63\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 4.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.60\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 5.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.51\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Image: 6.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.73\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Image: 7.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.67\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 8.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.72\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 9\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 9.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.70\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Image: 10.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.54\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 11.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.54\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 12\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 12.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 13\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 13.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 14\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 14.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.95\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 15\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 15.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 16\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Image: 16.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.65\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 17\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 17.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 18\n",
            "No image found with number 18\n",
            "Enter an image number (1-1000) or 'q' to quit: 19\n",
            "No image found with number 19\n",
            "Enter an image number (1-1000) or 'q' to quit: 18\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 18.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 19\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 19.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Image: 20.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 21\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Image: 21.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 22\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 22.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 23\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 23.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.98\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 24\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 24.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 25.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 26\n",
            "No image found with number 26\n",
            "Enter an image number (1-1000) or 'q' to quit: 26\n",
            "No image found with number 26\n",
            "Enter an image number (1-1000) or 'q' to quit: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 1.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 2.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 3.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.73\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Image: 4.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 5.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.98\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Image: 6.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.97\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 7.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Image: 8.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.84\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 9\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 9.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Image: 10.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 11\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Image: 11.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.96\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 12\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 12.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.83\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 13\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Image: 13.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 14\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 14.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.78\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 15\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Image: 15.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.37\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 16\n",
            "No image found with number 16\n",
            "Enter an image number (1-1000) or 'q' to quit: 16\n",
            "No image found with number 16\n",
            "Enter an image number (1-1000) or 'q' to quit: 17\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 17.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 18\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 18.jpg\n",
            "Predicted class: BCC\n",
            "Confidence: 0.66\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 19\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 19.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 20.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.98\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 21\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Image: 21.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.61\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 22\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 22.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 23\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Image: 23.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.98\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 24\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 24.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.93\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 25.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.50\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 26\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Image: 26.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.35\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 27\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Image: 27.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 28\n",
            "No image found with number 28\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a1a81ad8a993>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Interactive prediction loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter an image number (1-1000) or 'q' to quit: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION\n"
      ],
      "metadata": {
        "id": "5sf6jDTA1XLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Define the F1Score metric (same as in your training code)\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/drive/My Drive/models/skin_lesion_classifier_vgg16_all_classes.h5',\n",
        "                   custom_objects={'F1Score': F1Score})\n",
        "\n",
        "# Set up constants\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define the path to the images folder\n",
        "data_dir = '/content/drive/My Drive/images'\n",
        "\n",
        "# Set up data generator for evaluation\n",
        "eval_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "eval_generator = eval_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(eval_generator, verbose=1)\n",
        "\n",
        "# Print the results\n",
        "print(\"Evaluation on the entire dataset:\")\n",
        "for metric, score in zip(model.metrics_names, scores):\n",
        "    print(f\"{metric}: {score}\")\n",
        "\n",
        "# If you want to evaluate on specific classes\n",
        "class_names = ['MEL', 'NV', 'BCC', 'SCC']\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_generator = eval_datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        classes=[class_name],\n",
        "        shuffle=False\n",
        "    )\n",
        "    scores = model.evaluate(class_generator, verbose=0)\n",
        "    print(f\"\\nEvaluation on {class_name} class:\")\n",
        "    for metric, score in zip(model.metrics_names, scores):\n",
        "        print(f\"{metric}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "ZX2VNcV21Z3x",
        "outputId": "592927ce-379b-4eeb-8022-ac7bbb48d40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8985 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2561s\u001b[0m 9s/step - accuracy: 0.0680 - f1_score: 0.0437 - loss: 3.1579 - precision: 0.0470 - recall: 0.0409\n",
            "Evaluation on the entire dataset:\n",
            "loss: 3.615205764770508\n",
            "compile_metrics: 0.11285475641489029\n",
            "Found 1113 images belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node LogicalAnd defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [1,32] vs. [1,128]\n\t [[{{node LogicalAnd}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_12831[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_12930]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e601e3bdca0a>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     )\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEvaluation on {class_name} class:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node LogicalAnd defined at (most recent call last):\n<stack traces unavailable>\nIncompatible shapes: [1,32] vs. [1,128]\n\t [[{{node LogicalAnd}}]]\n\ttf2xla conversion failed while converting __inference_one_step_on_data_12831[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_12930]"
          ]
        }
      ]
    }
  ]
}