{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l1XcFYtXXPq",
        "outputId": "d0179c98-6994-41d5-b4ca-5246e847e86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Enter the path to the model file (or 'q' to quit): from google.colab import drive drive.mount('/content/drive')\n",
            "Error: File not found at from google.colab import drive drive.mount('/content/drive')\n",
            "Enter the path to the model file (or 'q' to quit): /content/drive/My Drive/models/skin_lesion_classifier_resnet50_final.keras\n",
            "Found 8985 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5174s\u001b[0m 18s/step - accuracy: 0.9193 - f1_score: 0.9192 - loss: 0.2455 - precision: 0.9195 - recall: 0.9189\n",
            "\n",
            "Evaluation results:\n",
            "loss: 0.37821459770202637\n",
            "compile_metrics: 0.8888146877288818\n",
            "Found 1113 images belonging to 1 classes.\n",
            "An error occurred: Graph execution error:\n",
            "\n",
            "Detected at node LogicalAnd defined at (most recent call last):\n",
            "<stack traces unavailable>\n",
            "Incompatible shapes: [1,32] vs. [1,128]\n",
            "\t [[{{node LogicalAnd}}]]\n",
            "\ttf2xla conversion failed while converting __inference_one_step_on_data_23081[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
            "\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_23764]\n",
            "Enter the path to the model file (or 'q' to quit): q\n",
            "Evaluation complete. Thank you!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')# Custom F1Score metric (make sure this matches the one used in training)\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision()\n",
        "        self.recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "# Set up constants\n",
        "IMG_SIZE = (224, 224)  # Adjust if your models use a different input size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define the path to the images folder\n",
        "data_dir = '/content/drive/My Drive/images'  # Update this path if necessary\n",
        "\n",
        "# Define class names\n",
        "class_names = ['MEL', 'NV', 'BCC', 'SCC']\n",
        "\n",
        "def evaluate_model(model_path):\n",
        "    # Load the model\n",
        "    model = load_model(model_path, custom_objects={'F1Score': F1Score})\n",
        "\n",
        "    # Set up data generator for evaluation\n",
        "    eval_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    eval_generator = eval_datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        classes=class_names,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    scores = model.evaluate(eval_generator, verbose=1)\n",
        "\n",
        "    # Print the results\n",
        "    print(\"\\nEvaluation results:\")\n",
        "    for metric, score in zip(model.metrics_names, scores):\n",
        "        print(f\"{metric}: {score}\")\n",
        "\n",
        "    # Evaluate on specific classes\n",
        "    for class_name in class_names:\n",
        "        class_generator = eval_datagen.flow_from_directory(\n",
        "            data_dir,\n",
        "            target_size=IMG_SIZE,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            classes=[class_name],\n",
        "            shuffle=False\n",
        "        )\n",
        "        scores = model.evaluate(class_generator, verbose=0)\n",
        "        print(f\"\\nEvaluation on {class_name} class:\")\n",
        "        for metric, score in zip(model.metrics_names, scores):\n",
        "            print(f\"{metric}: {score}\")\n",
        "\n",
        "# Main loop\n",
        "while True:\n",
        "    model_path = input(\"Enter the path to the model file (or 'q' to quit): \")\n",
        "\n",
        "    if model_path.lower() == 'q':\n",
        "        break\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: File not found at {model_path}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        evaluate_model(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "print(\"Evaluation complete. Thank you!\")"
      ]
    }
  ]
}