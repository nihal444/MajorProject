{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IpvDPFjThQl",
        "outputId": "7589fe2e-dcee-449c-91fc-56a9a510f38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Contents of data_dir:\n",
            "['NV', 'MEL', 'BCC', 'SCC']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up constants\n",
        "IMG_SIZE = (224, 224)  # MobileNetV2 default input size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "\n",
        "# Define the path to the images folder\n",
        "data_dir = '/content/drive/My Drive/images'\n",
        "print(\"Contents of data_dir:\")\n",
        "print(os.listdir(data_dir))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update class names to include SCC\n",
        "class_names = ['MEL', 'NV', 'BCC', 'SCC']\n",
        "for class_name in class_names:\n",
        "    if not os.path.isdir(os.path.join(data_dir, class_name)):\n",
        "        raise ValueError(f\"Folder {class_name} not found in {data_dir}\")\n",
        "\n",
        "# Set up data generators with increased augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    classes=class_names,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    classes=class_names,\n",
        "    shuffle=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bhICBL0Tlk8",
        "outputId": "31ab3303-9b99-4f6c-ff3c-c73388489bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7190 images belonging to 4 classes.\n",
            "Found 1795 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained MobileNetV2 model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE, 3))\n",
        "\n",
        "# Fine-tune the model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(4, activation='softmax')(x)  # 4 classes\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaD30mT1Tn3C",
        "outputId": "22c631ce-125b-44f4-e18f-ae3eeced8fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define F1 Score metric as a class\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "# Compile the model with additional metrics\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
        "\n"
      ],
      "metadata": {
        "id": "8ZvTk9kKTqbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Train the model with class weights\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/drive/My Drive/models/skin_lesion_classifier_mobilenetv2.h5')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DNz_frwTupc",
        "outputId": "200f8f06-b247-4ebc-d6a7-0fb2d6a91b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3468s\u001b[0m 15s/step - accuracy: 0.5436 - f1_score: 0.4759 - loss: 1.1110 - precision: 0.6288 - recall: 0.3871 - val_accuracy: 0.7500 - val_f1_score: 0.7502 - val_loss: 1.4346 - val_precision: 0.7504 - val_recall: 0.7500\n",
            "Epoch 2/30\n",
            "\u001b[1m  1/224\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 143ms/step - accuracy: 0.8438 - f1_score: 0.7797 - loss: 0.4789 - precision: 0.8519 - recall: 0.7188"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.8438 - f1_score: 0.7797 - loss: 0.4789 - precision: 0.8519 - recall: 0.7188 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 1.4237 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 3/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 1s/step - accuracy: 0.7319 - f1_score: 0.7276 - loss: 0.6001 - precision: 0.7711 - recall: 0.6889 - val_accuracy: 0.7500 - val_f1_score: 0.7502 - val_loss: 1.6652 - val_precision: 0.7504 - val_recall: 0.7500\n",
            "Epoch 4/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234us/step - accuracy: 0.8125 - f1_score: 0.7333 - loss: 0.3536 - precision: 0.7857 - recall: 0.6875 - val_accuracy: 0.3333 - val_f1_score: 0.3333 - val_loss: 3.9664 - val_precision: 0.3333 - val_recall: 0.3333\n",
            "Epoch 5/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 985ms/step - accuracy: 0.7596 - f1_score: 0.7578 - loss: 0.5026 - precision: 0.7886 - recall: 0.7294 - val_accuracy: 0.7522 - val_f1_score: 0.7522 - val_loss: 1.7336 - val_precision: 0.7522 - val_recall: 0.7522\n",
            "Epoch 6/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246us/step - accuracy: 0.7812 - f1_score: 0.8065 - loss: 0.4230 - precision: 0.8333 - recall: 0.7812 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 2.1812 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 7/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 969ms/step - accuracy: 0.7864 - f1_score: 0.7858 - loss: 0.4053 - precision: 0.8082 - recall: 0.7647 - val_accuracy: 0.7561 - val_f1_score: 0.7561 - val_loss: 1.4407 - val_precision: 0.7561 - val_recall: 0.7561\n",
            "Epoch 8/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217us/step - accuracy: 0.7812 - f1_score: 0.7937 - loss: 0.5586 - precision: 0.8065 - recall: 0.7812 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 1.9939 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 9/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 942ms/step - accuracy: 0.7860 - f1_score: 0.7833 - loss: 0.3933 - precision: 0.8034 - recall: 0.7643 - val_accuracy: 0.7589 - val_f1_score: 0.7599 - val_loss: 1.4394 - val_precision: 0.7613 - val_recall: 0.7584\n",
            "Epoch 10/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 176ms/step - accuracy: 0.7812 - f1_score: 0.7812 - loss: 0.3315 - precision: 0.7812 - recall: 0.7812 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 1.2088 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 11/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 951ms/step - accuracy: 0.8049 - f1_score: 0.8053 - loss: 0.3680 - precision: 0.8256 - recall: 0.7859 - val_accuracy: 0.7768 - val_f1_score: 0.7782 - val_loss: 1.2041 - val_precision: 0.7812 - val_recall: 0.7751\n",
            "Epoch 12/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209us/step - accuracy: 0.8750 - f1_score: 0.8750 - loss: 0.1803 - precision: 0.8750 - recall: 0.8750 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 0.9959 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 13/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 965ms/step - accuracy: 0.8152 - f1_score: 0.8126 - loss: 0.3275 - precision: 0.8292 - recall: 0.7967 - val_accuracy: 0.7896 - val_f1_score: 0.7929 - val_loss: 1.1449 - val_precision: 0.7974 - val_recall: 0.7885\n",
            "Epoch 14/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - accuracy: 0.8125 - f1_score: 0.7937 - loss: 0.1534 - precision: 0.8065 - recall: 0.7812 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 2.6813 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 15/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 958ms/step - accuracy: 0.8284 - f1_score: 0.8278 - loss: 0.3099 - precision: 0.8411 - recall: 0.8150 - val_accuracy: 0.7985 - val_f1_score: 0.8020 - val_loss: 0.9025 - val_precision: 0.8072 - val_recall: 0.7969\n",
            "Epoch 16/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 178ms/step - accuracy: 0.9062 - f1_score: 0.9032 - loss: 0.3096 - precision: 0.9333 - recall: 0.8750 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 2.2994 - val_precision: 0.6667 - val_recall: 0.6667\n",
            "Epoch 17/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 965ms/step - accuracy: 0.8388 - f1_score: 0.8398 - loss: 0.2926 - precision: 0.8507 - recall: 0.8292 - val_accuracy: 0.7991 - val_f1_score: 0.8004 - val_loss: 0.7566 - val_precision: 0.8063 - val_recall: 0.7946\n",
            "Epoch 18/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241us/step - accuracy: 0.8750 - f1_score: 0.8750 - loss: 0.1867 - precision: 0.8750 - recall: 0.8750 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 5.6886e-04 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 19/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 974ms/step - accuracy: 0.8410 - f1_score: 0.8435 - loss: 0.2725 - precision: 0.8546 - recall: 0.8327 - val_accuracy: 0.7952 - val_f1_score: 0.7947 - val_loss: 0.7417 - val_precision: 0.8056 - val_recall: 0.7840\n",
            "Epoch 20/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 160ms/step - accuracy: 0.9062 - f1_score: 0.9062 - loss: 0.1998 - precision: 0.9062 - recall: 0.9062 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0718 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 21/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 961ms/step - accuracy: 0.8578 - f1_score: 0.8572 - loss: 0.2506 - precision: 0.8677 - recall: 0.8470 - val_accuracy: 0.8198 - val_f1_score: 0.8187 - val_loss: 0.6587 - val_precision: 0.8261 - val_recall: 0.8114\n",
            "Epoch 22/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - accuracy: 0.7188 - f1_score: 0.7187 - loss: 0.2731 - precision: 0.7188 - recall: 0.7188 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0100 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 23/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 967ms/step - accuracy: 0.8438 - f1_score: 0.8431 - loss: 0.2562 - precision: 0.8527 - recall: 0.8337 - val_accuracy: 0.8097 - val_f1_score: 0.8120 - val_loss: 0.6266 - val_precision: 0.8217 - val_recall: 0.8025\n",
            "Epoch 24/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 171ms/step - accuracy: 0.9688 - f1_score: 0.9687 - loss: 0.1655 - precision: 0.9688 - recall: 0.9688 - val_accuracy: 0.3333 - val_f1_score: 0.4000 - val_loss: 0.6710 - val_precision: 0.5000 - val_recall: 0.3333\n",
            "Epoch 25/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 970ms/step - accuracy: 0.8604 - f1_score: 0.8608 - loss: 0.2274 - precision: 0.8692 - recall: 0.8525 - val_accuracy: 0.8443 - val_f1_score: 0.8422 - val_loss: 0.4824 - val_precision: 0.8491 - val_recall: 0.8354\n",
            "Epoch 26/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 171ms/step - accuracy: 0.9375 - f1_score: 0.9375 - loss: 0.1129 - precision: 0.9375 - recall: 0.9375 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0908 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 27/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 966ms/step - accuracy: 0.8576 - f1_score: 0.8561 - loss: 0.2287 - precision: 0.8636 - recall: 0.8488 - val_accuracy: 0.8181 - val_f1_score: 0.8194 - val_loss: 0.6699 - val_precision: 0.8248 - val_recall: 0.8142\n",
            "Epoch 28/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 169ms/step - accuracy: 0.9688 - f1_score: 0.9687 - loss: 0.0847 - precision: 0.9688 - recall: 0.9688 - val_accuracy: 1.0000 - val_f1_score: 1.0000 - val_loss: 0.0335 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 29/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 972ms/step - accuracy: 0.8727 - f1_score: 0.8693 - loss: 0.2274 - precision: 0.8779 - recall: 0.8609 - val_accuracy: 0.8488 - val_f1_score: 0.8479 - val_loss: 0.5754 - val_precision: 0.8527 - val_recall: 0.8432\n",
            "Epoch 30/30\n",
            "\u001b[1m224/224\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238us/step - accuracy: 0.8750 - f1_score: 0.8889 - loss: 0.1733 - precision: 0.9032 - recall: 0.8750 - val_accuracy: 0.6667 - val_f1_score: 0.6667 - val_loss: 0.3849 - val_precision: 0.6667 - val_recall: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print class indices\n",
        "# print(\"Class indices:\", train_generator.class_indices)\n",
        "model_saved = tf.keras.models.load_model('/content/drive/My Drive/models/skin_lesion_classifier_mobilenetv2.h5')\n",
        "# Update class names to include SCC\n",
        "class_names = ['MEL', 'NV', 'BCC', 'SCC']\n",
        "for class_name in class_names:\n",
        "    if not os.path.isdir(os.path.join(data_dir, class_name)):\n",
        "        raise ValueError(f\"Folder {class_name} not found in {data_dir}\")\n",
        "\n",
        "# Function to predict image\n",
        "def predict_image(img_path, nv_threshold=0.7):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.0\n",
        "\n",
        "    prediction = model_saved.predict(img_array)\n",
        "\n",
        "    if np.argmax(prediction) == class_names.index('NV') and prediction[0][class_names.index('NV')] < nv_threshold:\n",
        "        predicted_class = class_names[np.argsort(prediction[0])[-2]]\n",
        "    else:\n",
        "        predicted_class = class_names[np.argmax(prediction)]\n",
        "\n",
        "    confidence = np.max(prediction)\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "finish_dir = '/content/drive/My Drive/finish'\n",
        "# Interactive prediction loop\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVptHN21TxOi",
        "outputId": "5acc4401-9cd8-4360-f96e-5e2933c6751f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"Enter an image number (1-1000) or 'q' to quit: \")\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        image_number = int(user_input)\n",
        "\n",
        "        for filename in os.listdir(finish_dir):\n",
        "            if filename.startswith(f\"{image_number}.\") and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(finish_dir, filename)\n",
        "\n",
        "                predicted_class, confidence = predict_image(img_path)\n",
        "\n",
        "                print(f\"Image: {filename}\")\n",
        "                print(f\"Predicted class: {predicted_class}\")\n",
        "                print(f\"Confidence: {confidence:.2f}\")\n",
        "                print()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found with number {image_number}\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number or 'q' to quit.\")\n",
        "\n",
        "print(\"Thank you for using the classifier!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "TfaPqoeuTz05",
        "outputId": "b6238b2e-3fe5-416e-8fec-2ff003d087fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an image number (1-1000) or 'q' to quit: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "Image: 1.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.74\n",
            "\n",
            "Enter an image number (1-1000) or 'q' to quit: 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "Image: 2.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-41b776d62df0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter an image number (1-1000) or 'q' to quit: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,54):\n",
        "    user_input=i\n",
        "    try:\n",
        "        image_number = int(user_input)\n",
        "\n",
        "        for filename in os.listdir(finish_dir):\n",
        "            if filename.startswith(f\"{image_number}.\") and filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(finish_dir, filename)\n",
        "\n",
        "                predicted_class, confidence = predict_image(img_path)\n",
        "\n",
        "                print(f\"Image: {filename}\")\n",
        "                print(f\"Predicted class: {predicted_class}\")\n",
        "                print(f\"Confidence: {confidence:.2f}\")\n",
        "                print()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"No image found with number {image_number}\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(\"Invalid input\")\n",
        "\n",
        "print(\"Thank you for using the classifier!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYGbctNkg7KG",
        "outputId": "0594442f-6e4c-4eec-b333-af30910a6a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 1.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.74\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Image: 2.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "Image: 3.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 4.jpg\n",
            "Predicted class: SCC\n",
            "Confidence: 0.70\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 5.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "Image: 6.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.74\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "Image: 7.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.81\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "Image: 8.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "Image: 9.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "Image: 10.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "Image: 11.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 12.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 13.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.94\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
            "Image: 14.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
            "Image: 15.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.81\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Image: 16.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "Image: 17.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "Image: 18.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 19.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Image: 20.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Image: 21.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Image: 22.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.76\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 23.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Image: 24.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Image: 25.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.72\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Image: 26.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.97\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 27.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "Image: 28.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Image: 29.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Image: 30.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "Image: 31.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.55\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Image: 32.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
            "Image: 33.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.63\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
            "Image: 34.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "Image: 35.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 36.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.80\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Image: 37.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.98\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Image: 38.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Image: 39.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "Image: 40.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Image: 41.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.95\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Image: 42.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Image: 43.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "Image: 44.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "Image: 45.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.80\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "Image: 46.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 47.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Image: 48.jpg\n",
            "Predicted class: MEL\n",
            "Confidence: 0.53\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Image: 49.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.71\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 50.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.99\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Image: 51.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Image: 52.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 0.75\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Image: 53.jpg\n",
            "Predicted class: NV\n",
            "Confidence: 1.00\n",
            "\n",
            "Thank you for using the classifier!\n"
          ]
        }
      ]
    }
  ]
}